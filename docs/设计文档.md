# MegaLLM API 代理服务 - 接口设计文档

## API 请求方式

### 基本请求格式

本服务完全兼容 OpenAI API 格式，只需要提供 `messages` 参数即可使用。

#### 最简请求（使用默认模型）

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "你好！"}]
  }'
```

**说明**: 不传递 `model` 参数时，默认使用 `openai-gpt-oss-120b` 模型。

#### 指定模型

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "你好！"}]
  }'
```

#### 自定义参数

所有参数都是可选的，如果不传递，后端会使用默认值：

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "你好！"}],
    "model": "gpt-4",
    "temperature": 0.7,
    "max_tokens": 1000,
    "top_p": 0.9
  }'
```

### 支持的参数

| 参数 | 类型 | 必填 | 默认值 | 说明 |
|-----|------|------|--------|------|
| messages | array | ✅ | - | 聊天消息列表 |
| model | string | ❌ | openai-gpt-oss-120b | 模型ID |
| temperature | float | ❌ | 1.0 | 采样温度 (0-2) |
| top_p | float | ❌ | 1.0 | 核采样 (0-1) |
| n | integer | ❌ | 1 | 返回结果数量 (1-10) |
| stream | boolean | ❌ | false | 是否流式返回 |
| max_tokens | integer | ❌ | - | 最大token数 |
| presence_penalty | float | ❌ | 0.0 | 存在惩罚 (-2 to 2) |
| frequency_penalty | float | ❌ | 0.0 | 频率惩罚 (-2 to 2) |

### 测试多个模型

```bash
# 使用模型 ID 测试多个模型
for model in "gpt-4" "openai-gpt-oss-120b" "llama3.3-70b-instruct"; do
  echo "测试 $model..."
  curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d "{
      \"model\": \"$model\",
      \"messages\": [{\"role\": \"user\", \"content\": \"你好！\"}]
    }"
done
```

### 上游 MegaLLM API 请求方式

本服务代理的上游 MegaLLM API 请求格式：

```bash
curl https://ai.megallm.io/v1/chat/completions \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "你好！"}]
  }'
```

**注意**: 使用本代理服务时，不需要提供 API Key，服务会自动管理密钥轮询。