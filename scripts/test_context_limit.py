"""
æµ‹è¯•ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶åŠŸèƒ½
"""
import requests
import json


def test_get_model_info():
    """æµ‹è¯•è·å–æ¨¡å‹ä¿¡æ¯"""
    print("=" * 60)
    print("æµ‹è¯• 1: è·å–æŒ‡å®šæ¨¡å‹ä¿¡æ¯")
    print("=" * 60)

    models_to_test = [
        "openai-gpt-oss-120b",
        "qwen/qwen3-next-80b-a3b-instruct",
        "moonshotai/kimi-k2-instruct-0905",
        "llama3-8b-instruct"
    ]

    for model in models_to_test:
        url = f"http://localhost:8000/v1/models/{model}/info"
        try:
            response = requests.get(url)
            response.raise_for_status()
            info = response.json()

            print(f"\næ¨¡å‹: {info['id']}")
            print(f"  ä¸Šä¸‹æ–‡é•¿åº¦: {info['context_length_formatted']}")
            if 'usable_limit_formatted' in info:
                print(f"  å¯ç”¨é™åˆ¶ (90%): {info['usable_limit_formatted']}")
            if 'note' in info:
                print(f"  å¤‡æ³¨: {info['note']}")

        except requests.exceptions.RequestException as e:
            print(f"\nâŒ è·å–æ¨¡å‹ {model} ä¿¡æ¯å¤±è´¥: {e}")


def test_context_within_limit():
    """æµ‹è¯•æ­£å¸¸å¯¹è¯ï¼ˆåœ¨ä¸Šä¸‹æ–‡é™åˆ¶å†…ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: æ­£å¸¸å¯¹è¯ï¼ˆä¸Šä¸‹æ–‡åœ¨é™åˆ¶å†…ï¼‰")
    print("=" * 60)

    url = "http://localhost:8000/v1/chat/completions"

    payload = {
        "model": "llama3-8b-instruct",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"},
            {"role": "user", "content": "ä½ å¥½"},
            {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"},
            {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹Python"}
        ]
    }

    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()

        result = response.json()
        print("\nâœ… è¯·æ±‚æˆåŠŸ")
        print(f"æ¨¡å‹: {result.get('model', 'N/A')}")
        print(f"å“åº”: {result['choices'][0]['message']['content'][:100]}...")

    except requests.exceptions.RequestException as e:
        print(f"\nâŒ è¯·æ±‚å¤±è´¥: {e}")
        if hasattr(e, 'response') and e.response is not None:
            print(f"å“åº”å†…å®¹: {e.response.text}")


def test_context_exceeded():
    """æµ‹è¯•è¶…å‡ºä¸Šä¸‹æ–‡é™åˆ¶"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: è¶…å‡ºä¸Šä¸‹æ–‡é™åˆ¶")
    print("=" * 60)

    url = "http://localhost:8000/v1/chat/completions"

    # ç”Ÿæˆä¸€ä¸ªå¾ˆé•¿çš„å¯¹è¯å†å²
    messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"}]

    # æ·»åŠ å¾ˆå¤šè½®å¯¹è¯ï¼ˆæ¯è½®çº¦ 100 ä¸ª tokensï¼‰
    # llama3-8b-instruct çš„ä¸Šä¸‹æ–‡æ˜¯ 8000 tokensï¼Œç”Ÿæˆ 100 è½®åº”è¯¥ä¼šè¶…é™
    for i in range(100):
        messages.append({
            "role": "user",
            "content": f"è¿™æ˜¯ç¬¬ {i} è½®å¯¹è¯ï¼Œè¯·å›ç­”ä¸€äº›å…³äºäººå·¥æ™ºèƒ½çš„é—®é¢˜ã€‚äººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ"
        })
        messages.append({
            "role": "assistant",
            "content": f"è¿™æ˜¯ç¬¬ {i} è½®å›ç­”ã€‚äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"
        })

    payload = {
        "model": "llama3-8b-instruct",
        "messages": messages
    }

    print(f"\nå‘é€ {len(messages)} æ¡æ¶ˆæ¯...")

    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()

        result = response.json()
        print("\nâš ï¸  æ„å¤–ï¼šè¯·æ±‚æˆåŠŸäº†ï¼ˆå¯èƒ½æ˜¯ä¼°ç®—ä¸å‡†ç¡®ï¼‰")
        print(f"æ¨¡å‹: {result.get('model', 'N/A')}")

    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 400:
            error_detail = e.response.json()
            error_info = error_detail.get("detail", {}).get("error", {})

            print("\nâœ… æ­£ç¡®æ•è·ä¸Šä¸‹æ–‡è¶…é™é”™è¯¯")
            print(f"é”™è¯¯ç±»å‹: {error_info.get('type', 'N/A')}")
            print(f"é”™è¯¯ä¿¡æ¯: {error_info.get('message', 'N/A')}")

            if 'param' in error_info:
                param = error_info['param']
                print(f"\nè¯¦ç»†ä¿¡æ¯:")
                print(f"  æ¨¡å‹: {param.get('model', 'N/A')}")
                print(f"  å½“å‰ tokens: {param.get('current_tokens', 'N/A'):,}")
                print(f"  ä¸Šé™ tokens: {param.get('limit', 'N/A'):,}")
                print(f"  ä½¿ç”¨ç‡: {param.get('usage_percentage', 'N/A')}%")
        else:
            print(f"\nâŒ æ„å¤–çš„ HTTP é”™è¯¯: {e}")
            print(f"å“åº”å†…å®¹: {e.response.text}")

    except Exception as e:
        print(f"\nâŒ è¯·æ±‚å¤±è´¥: {e}")


def test_get_all_models():
    """æµ‹è¯•è·å–æ‰€æœ‰æ¨¡å‹åˆ—è¡¨"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: è·å–æ‰€æœ‰æ¨¡å‹åˆ—è¡¨ï¼ˆå¸¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰")
    print("=" * 60)

    url = "http://localhost:8000/v1/models"

    try:
        response = requests.get(url)
        response.raise_for_status()

        result = response.json()
        models = result.get("data", [])

        print(f"\nå…± {len(models)} ä¸ªæ¨¡å‹:\n")

        # æŒ‰ä¸Šä¸‹æ–‡é•¿åº¦æ’åº
        sorted_models = sorted(
            models,
            key=lambda x: x.get("context_length", 0),
            reverse=True
        )

        for model in sorted_models:
            model_id = model.get("id", "N/A")
            context = model.get("context_length_formatted", "æœªçŸ¥")
            print(f"  {model_id:<50} {context}")

    except requests.exceptions.RequestException as e:
        print(f"\nâŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")


if __name__ == "__main__":
    print("\nğŸ§ª å¼€å§‹æµ‹è¯•ä¸Šä¸‹æ–‡é™åˆ¶åŠŸèƒ½\n")

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_get_model_info()
    test_context_within_limit()
    test_context_exceeded()
    test_get_all_models()

    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
    print("=" * 60)
